"""

Module that configures and initializes the language model for both text and function-calling modes.

This module sets up the language model (LLM) based on the interpreter's settings. If the interpreter is
configured for function calling, it will initialize an OpenAI coding language model. Otherwise, it sets up
a text language model and wraps it for coding purposes. The module additionally adjusts settings related to
function-calling capabilities, LLM's local or cloud execution, and vision capabilities for GPT-4.

Functions:
    setup_llm(interpreter: Interpreter) -> Callable:
        Initializes and returns a coding_llm function based on the interpreter's configuration.

        Args:
            interpreter (Interpreter): The interpreter instance with configuration settings for the LLM.

        Returns:
            Callable: A function that takes a list of messages and returns a generator yielding outputs
            from the language model. The output format may vary depending on whether the LLM is set for text
            or function-calling.

        Raises:
            TypeError: In case of an unexpected type error during message trimming for vision tasks.

Note: Documentation automatically generated by https://undoc.ai
"""
import os

import litellm

from .convert_to_coding_llm import convert_to_coding_llm
from .setup_openai_coding_llm import setup_openai_coding_llm
from .setup_text_llm import setup_text_llm


def setup_llm(interpreter):
    """
    def setup_llm(interpreter):
        Sets up the appropriate language learning model (LLM) for the interpreter.
        This function determines whether to utilize a function calling LLM or a non-function calling LLM
        based on the properties of the passed interpreter object. If the interpreter's `function_calling_llm`
        attribute is `None`, the function assesses the local environment, model details, and other factors
        to decide the LLM type to be used. The function then proceeds to initialize either a coding LLM or
        a text-based LLM as per the assessment.
        Args:
            interpreter (Interpreter): An instance of the interpreter object containing configuration
                settings like model details, local environment flags, etc.
        Returns:
            function: A function representing the initialized language learning model suited for
                the current interpreter setup.
        Raises:
            TypeError: If an unexpected type mismatch occurs during LLM setup.
            Other possible exceptions related to the interpreter's internal state and configuration.
    """

    # Detect whether or not it's a function calling LLM
    if interpreter.function_calling_llm == None:
        if not interpreter.local and (
            interpreter.model != "gpt-4-vision-preview"
            and interpreter.model in litellm.open_ai_chat_completion_models
            or interpreter.model.startswith("azure/")
        ):
            interpreter.function_calling_llm = True
        else:
            interpreter.function_calling_llm = False

    if interpreter.function_calling_llm:
        # Function-calling LLM
        coding_llm = setup_openai_coding_llm(interpreter)
    else:
        # If disable_procedures has not been set manually:
        if interpreter.disable_procedures == None:
            # Disable procedures, which confuses most of these models (except GPT-4V)

            if interpreter.model != "gpt-4-vision-preview":
                interpreter.disable_procedures = True

        # Non-function-calling LLM
        text_llm = setup_text_llm(interpreter)
        coding_llm = convert_to_coding_llm(text_llm, debug_mode=interpreter.debug_mode)

    return coding_llm
