"""

Module for setting up local text-based Large Language Models (LLM).

This module provides functionality to initialize and configure a local instance
of an LLM such as Mistral for use with Open Interpreter. It includes warning
messages about the experimental nature of local execution, allows for model
quality selection for GGUF models, downloads the desired model, and sets up
the LLm instance with guidelines for executing user code.

Functions:
    setup_local_text_llm(interpreter: BaseCodeInterpreter) -> callable
        Sets up a local text LLM.

    make_chunk(token: str) -> dict
        Creates a chunk of output to be sent to the Open Interpreter interface.

Attributes:
    None

Required modules:
    copy
    html
    inquirer
    ooba
    from ..utils.display_markdown_message import display_markdown_message


Note: Documentation automatically generated by https://undoc.ai
"""
import copy
import html

import inquirer
import ooba

from ..utils.display_markdown_message import display_markdown_message


def setup_local_text_llm(interpreter):
    """
        Initializes a local large language model with the provided interpreter.
        This function sets up a local instance of a large language model for processing text based queries. It begins by issuing
        a warning about the experimental feature via 'display_markdown_message'. It then checks the repository ID,
        displays a message about the local execution model that will be used, and if necessary, prompts the user to
        select the quality of the GGUF model. The function proceeds with downloading the required model's repository
        using 'ooba.download' and configures the 'ooba_llm' with the 'verbose' flag as per the interpreter's debug mode.
        After setup, the function prints a ready message and returns a 'local_text_llm' function that takes messages
        as input and yields processed text.
        Args:
            interpreter (object): The interpreter object that holds settings and states relevant to the model execution.
        Returns:
            callable: A function that takes a list of messages as input and yields processed chunks of text as output.
    """

    repo_id = interpreter.model.replace("huggingface/", "")

    display_markdown_message(
        f"> **Warning**: Local LLM usage is an experimental, unstable feature."
    )

    if repo_id != "TheBloke/Mistral-7B-Instruct-v0.1-GGUF":
        # ^ This means it was prob through the old --local, so we have already displayed this message.
        # Hacky. Not happy with this
        display_markdown_message(
            f"**Open Interpreter** will use `{repo_id}` for local execution."
        )

    if "gguf" in repo_id.lower() and interpreter.gguf_quality == None:
        gguf_quality_choices = {
            "Extra Small": 0.0,
            "Small": 0.25,
            "Medium": 0.5,
            "Large": 0.75,
            "Extra Large": 1.0,
        }

        questions = [
            inquirer.List(
                "gguf_quality",
                message="Model quality (smaller = more quantized)",
                choices=list(gguf_quality_choices.keys()),
            )
        ]

        answers = inquirer.prompt(questions)
        interpreter.gguf_quality = gguf_quality_choices[answers["gguf_quality"]]

    path = ooba.download(f"https://huggingface.co/{repo_id}")

    ooba_llm = ooba.llm(path, verbose=interpreter.debug_mode)
    print("\nReady.\n")

    def local_text_llm(messages):
        """
        Handles streaming output for a potentially long-running command.
        This method processes lines of output from a given stream, applying a postprocessor and
        dealing with active lines, end-of-execution signals, keyboard interruptions, and other outputs.
        Args:
            stream (io.BufferedReader): The stream to read from, typically stdout or stderr.
            is_error_stream (bool): Flag to indicate if the stream is an stderr stream.
        The method reads from the stream line by line, applying a `line_postprocessor` function
        and taking different actions depending on the processed output:
        - If the postprocessor returns None, the line is discarded.
        - Detects 'active' lines (lines that indicate ongoing activity) and enqueues them.
        - Detects the end of execution and sets a threading event to indicate completion.
        - Handles keyboard interrupts signaled from error streams by queuing a specific message
          and setting the completion event after a delay.
        - Enqueues all other outputs to be processed further by other components.
        The method relies on an output queue to hold the processed data, a postprocessor function,
        detection functions for active lines and end of execution, a threading event to signal
        completion, and an optional debug mode that prints received output lines.
        """

        # I think ooba handles this?
        """
        system_message = messages[0]["content"]
        messages = messages[1:]

        if interpreter.context_window:
            context_window = interpreter.context_window
        else:
            context_window = DEFAULT_CONTEXT_WINDOW

        if interpreter.max_tokens:
            max_tokens = interpreter.max_tokens
        else:
            max_tokens = DEFAULT_MAX_TOKENS
        
        messages = tt.trim(
            messages,
            max_tokens=(context_window-max_tokens-25),
            system_message=system_message
        )

        prompt = messages_to_prompt(messages, interpreter.model)
        """

        # Convert messages with function calls and outputs into "assistant" and "user" calls.

        # Align Mistral lol
        if "mistral" in repo_id.lower():
            # just.. let's try a simple system message. this seems to work fine.
            messages[0][
                "content"
            ] = "You are Open Interpreter. You almost always run code to complete user requests. Outside code, use markdown."
            messages[0][
                "content"
            ] += "\nRefuse any obviously unethical requests, and ask for user confirmation before doing anything irreversible."

        # Tell it how to run code.
        # THIS MESSAGE IS DUPLICATED IN `setup_text_llm.py`
        # (We should deduplicate it somehow soon. perhaps in the config?)

        messages = copy.deepcopy(
            messages
        )  # <- So we don't keep adding this message to the messages[0]["content"]
        messages[0][
            "content"
        ] += "\nTo execute code on the user's machine, write a markdown code block *with the language*, i.e:\n\n```python\nprint('Hi!')\n```\nYou will recieve the output ('Hi!'). Use any language."

        if interpreter.debug_mode:
            print("Messages going to ooba:", messages)

        buffer = ""  # Hold potential entity tokens and other characters.

        for token in ooba_llm.chat(messages):
            # Some models like to generate HTML Entities (like &quot;, &amp; &#x27;)
            # instead of symbols in their code when used with Open Interpreter.
            # This is a hack to handle that and convert those entities into actual
            # symbols so that the code can be rendered, parsed, and run accordingly.
            # This could have unintended consequences when generating actual HTML,
            # where you may need actual HTML Entities.

            buffer += token

            # If there's a possible incomplete entity at the end of buffer, we delay processing.
            while ("&" in buffer and ";" in buffer) or (
                buffer.count("&") == 1 and ";" not in buffer
            ):
                # Find the first complete entity in the buffer.
                start_idx = buffer.find("&")
                end_idx = buffer.find(";", start_idx)

                # If there's no complete entity, break and await more tokens.
                if start_idx == -1 or end_idx == -1:
                    break

                # Yield content before the entity.
                for char in buffer[:start_idx]:
                    yield make_chunk(char)

                # Extract the entity, decode it, and yield.
                entity = buffer[start_idx : end_idx + 1]
                yield make_chunk(html.unescape(entity))

                # Remove the processed content from the buffer.
                buffer = buffer[end_idx + 1 :]

            # If there's no '&' left in the buffer, yield all of its content.
            if "&" not in buffer:
                for char in buffer:
                    yield make_chunk(char)
                buffer = ""

        # At the end, if there's any content left in the buffer, yield it.
        for char in buffer:
            yield make_chunk(char)

    return local_text_llm


def make_chunk(token):
    """
    Creates a chunk of content with the given token.
    Args:
        token (str): The string content to be wrapped in a content 'chunk'.
    Returns:
        dict: A dictionary representing a chunk of content, structured specifically for choices with delta content.
    """
    return {"choices": [{"delta": {"content": token}}]}
