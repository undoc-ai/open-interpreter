"""

Module containing the 'setup_openai_coding_llm' function to initialize the Coding Language Model (LLM) for use with OpenAI's API.

This module provides a setup function to configure a Coding Language Model with additional functionality like message trimming, handling function calls, and managing tokens. The function 'setup_openai_coding_llm' prepares the LLM for execution of user code on their local machine.

Functions:
    def setup_openai_coding_llm(interpreter):
        Initializes and returns the Coding Language Model (LLM) tailored to OpenAI's API requirements.

        This function prepares a callable Coding LLM object that receives messages, process them as per the requirements of OpenAI's API, and yields the resulting output. It maintains and adjusts tokens based on interpreter settings, handles system messages, and facilitates the execution of the provided `execute` function.

        Args:
            interpreter: An interpreter object which holds settings and configurations for the LLM interaction.

        Returns:
            A function object which can be called with messages to obtain output from the initialized LLM.

Note: Documentation automatically generated by https://undoc.ai
"""
import litellm
import tokentrim as tt

from ...terminal_interface.utils.display_markdown_message import (
    display_markdown_message,
)
from ..utils.convert_to_openai_messages import convert_to_openai_messages
from ..utils.merge_deltas import merge_deltas
from ..utils.parse_partial_json import parse_partial_json

function_schema = {
    "name": "execute",
    "description": "Executes code on the user's machine, **in the users local environment**, and returns the output",
    "parameters": {
        "type": "object",
        "properties": {
            "language": {
                "type": "string",
                "description": "The programming language (required parameter to the `execute` function)",
                "enum": [
                    "python",
                    "R",
                    "shell",
                    "applescript",
                    "javascript",
                    "html",
                    "powershell",
                ],
            },
            "code": {"type": "string", "description": "The code to execute (required)"},
        },
        "required": ["language", "code"],
    },
}


def setup_openai_coding_llm(interpreter):
    """
    Creates a coding language model generator for OpenAI's coding models.
    The function `setup_openai_coding_llm` initializes a coding language model generator function specifically suited to OpenAI's coding models.
    It wraps the provided `interpreter` object's internal state and functionality into a callable generator that processes input messages and yields responses based on the behavior of an underlying language model.
    Args:
        interpreter (object): An object that encapsulates the state and behavior needed to interact with the OpenAI API.
        The interpreter is expected to have properties such as `model`, `context_window`, `debug_mode`, and optionally, API specifics like `api_base`, `api_key`, `api_version`, `max_tokens`, `temperature`, and `max_budget`.
    Returns:
        Callable[[List[Dict[str, Any]]], Generator[Dict[str, Any], None, None]]: A generator function that takes in a list of messages (usually in the form of dialogue exchanges) and yields dictionaries containing various response elements like accumulated code deltas or newly detected programming languages based on the input messages and the model's responses.
    The generator function internally:
        - Formats messages to be compatible with OpenAI's API.
        - Trims the input messages to fit the context window size.
        - Adds system messages and function calls to the input.
        - Handles API interaction and response parsing.
        - Accumulates deltas (changes inferred by the model) to build code responses.
        - Switches languages or yields code based on the detected programming language in the input.
        - Reports debug information if the interpreter is in debug mode.
    The yields from the generator function reflect the incremental output from the model's responses, allowing a calling routine to handle code generation or execution as it unfolds in real-time.
    This function is designed to work with the LiteLLM API client library and requires it to be properly installed and configured in the environment where this function will be used.
    """

    def coding_llm(messages):
        """
        Sends messages to the OpenAI language model, optionally handling function calls, and yields processed output.
        This function initiates the process of communication with an instance of the OpenAI language model. It first formats input messages
        into a structure that is compatible with OpenAI's requirements, ensuring the existence of a system message and handling function
        call syntax as required. Messages are then trimmed to fit the specified context window or the model's default limits. The function
        sends the prepared messages to the language model and as it receives responses, it processes and accumulates deltas that include
        text, language, and code. The response output is provided as a generator that yields dictionary objects reflecting different
        types of content such as language or code segments.
        Args:
            messages (list of dict): A list of message dictionaries to be processed and sent to the language model.
        Yields:
            dict: Dictionary objects containing 'message', 'language', or 'code' keys along with their respective content,
                  extracted from the language model's response.
        """
        # Convert messages
        messages = convert_to_openai_messages(messages, function_calling=True)

        # Add OpenAI's recommended function message
        messages[0][
            "content"
        ] += "\n\nOnly use the function you have been provided with."

        # Seperate out the system_message from messages
        # (We expect the first message to always be a system_message)
        system_message = messages[0]["content"]
        messages = messages[1:]

        # Trim messages, preserving the system_message
        try:
            messages = tt.trim(
                messages=messages,
                system_message=system_message,
                model=interpreter.model,
            )
        except:
            if interpreter.context_window:
                messages = tt.trim(
                    messages=messages,
                    system_message=system_message,
                    max_tokens=interpreter.context_window,
                )
            else:
                if len(messages) == 1:
                    display_markdown_message(
                        """
                    **We were unable to determine the context window of this model.** Defaulting to 3000.
                    If your model can handle more, run `interpreter --context_window {token limit}` or `interpreter.context_window = {token limit}`.
                    """
                    )
                messages = tt.trim(
                    messages=messages, system_message=system_message, max_tokens=3000
                )

        if interpreter.debug_mode:
            print("Sending this to the OpenAI LLM:", messages)

        # Create LiteLLM generator
        params = {
            "model": interpreter.model,
            "messages": messages,
            "stream": True,
            "functions": [function_schema],
        }

        # Optional inputs
        if interpreter.api_base:
            params["api_base"] = interpreter.api_base
        if interpreter.api_key:
            params["api_key"] = interpreter.api_key
        if interpreter.api_version:
            params["api_version"] = interpreter.api_version
        if interpreter.max_tokens:
            params["max_tokens"] = interpreter.max_tokens
        if interpreter.temperature is not None:
            params["temperature"] = interpreter.temperature
        else:
            params["temperature"] = 0.0

        # These are set directly on LiteLLM
        if interpreter.max_budget:
            litellm.max_budget = interpreter.max_budget
        if interpreter.debug_mode:
            litellm.set_verbose = True

        # Report what we're sending to LiteLLM
        if interpreter.debug_mode:
            print("Sending this to LiteLLM:", params)

        response = litellm.completion(**params)

        accumulated_deltas = {}
        language = None
        code = ""

        for chunk in response:
            if interpreter.debug_mode:
                print("Chunk from LLM", chunk)

            if "choices" not in chunk or len(chunk["choices"]) == 0:
                # This happens sometimes
                continue

            delta = chunk["choices"][0]["delta"]

            # Accumulate deltas
            accumulated_deltas = merge_deltas(accumulated_deltas, delta)

            if interpreter.debug_mode:
                print("Accumulated deltas", accumulated_deltas)

            if "content" in delta and delta["content"]:
                yield {"message": delta["content"]}

            if (
                "function_call" in accumulated_deltas
                and "arguments" in accumulated_deltas["function_call"]
            ):
                if (
                    "name" in accumulated_deltas["function_call"]
                    and accumulated_deltas["function_call"]["name"] == "execute"
                ):
                    arguments = accumulated_deltas["function_call"]["arguments"]
                    arguments = parse_partial_json(arguments)

                    if arguments:
                        if (
                            language is None
                            and "language" in arguments
                            and "code"
                            in arguments  # <- This ensures we're *finished* typing language, as opposed to partially done
                            and arguments["language"]
                        ):
                            language = arguments["language"]
                            yield {"language": language}

                        if language is not None and "code" in arguments:
                            # Calculate the delta (new characters only)
                            code_delta = arguments["code"][len(code) :]
                            # Update the code
                            code = arguments["code"]
                            # Yield the delta
                            if code_delta:
                                yield {"code": code_delta}
                    else:
                        if interpreter.debug_mode:
                            print("Arguments not a dict.")

                # 3.5 REALLY likes to halucinate a function named `python` and you can't really fix that, it seems.
                # We just need to deal with it.
                elif (
                    "name" in accumulated_deltas["function_call"]
                    and accumulated_deltas["function_call"]["name"] == "python"
                ):
                    if interpreter.debug_mode:
                        print("Got direct python call")
                    if language is None:
                        language = "python"
                        yield {"language": language}

                    if language is not None:
                        # Pull the code string straight out of the "arguments" string
                        code_delta = accumulated_deltas["function_call"]["arguments"][
                            len(code) :
                        ]
                        # Update the code
                        code = accumulated_deltas["function_call"]["arguments"]
                        # Yield the delta
                        if code_delta:
                            yield {"code": code_delta}

                else:
                    # If name exists and it's not "execute" or "python", who knows what's going on.
                    if "name" in accumulated_deltas["function_call"]:
                        print(
                            "Encountered an unexpected function call: ",
                            accumulated_deltas["function_call"],
                            "\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter",
                        )

    return coding_llm
