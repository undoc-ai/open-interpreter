"""



Module: core

This module provides the core functionality for the Open Interpreter interpreter class. The Interpreter class allows for setting up, configuring, customizing, and running chat sessions with a large language model (LLM) that can execute code. The Interpreter class includes methods for starting a terminal interface, managing conversation history, and handling streaming chat sessions with the LLM.

Classes:
    Interpreter:
        Initializes a new Interpreter instance with configurable properties and extends it with external configuration settings. The class manages the conversation state, establishes settings for LLM interaction, enables interactive chats, and executes code in supported languages using appropriate code interpreters.

        The class provides methods for extending configuration from a given path, resetting its own state, and generating system messages for the LLM session. The key chat method leverages an internal streaming chat function to communicate with the LLM in real-time or a non-streaming mode, handling message processing, code execution, and response streaming. Conversation history tracking and file management are also supported features of the class.

Note: Documentation automatically generated by https://undoc.ai
"""

import json
import os
from datetime import datetime

from ..terminal_interface.start_terminal_interface import start_terminal_interface
from ..terminal_interface.terminal_interface import terminal_interface
from ..terminal_interface.utils.get_config import get_config, user_config_path
from ..terminal_interface.utils.local_storage_path import get_storage_path
from .generate_system_message import generate_system_message
from .llm.setup_llm import setup_llm
from .respond import respond


class Interpreter:
    """
    An Interpreter class responsible for orchestrating an interactive terminal experience using language model-based conversation handlers and custom interpreter logic.
    Attributes:
        messages (list): A list to store messages as part of the conversation history.
        _code_interpreters (dict): A dictionary to hold code interpreter instances.
        config_file (str): Path to the user's configuration file.
        local (bool): Flag to determine if the interpreter operates in a local environment.
        auto_run (bool): Flag to decide if code should be automatically executed.
        debug_mode (bool): Enables verbose output for debugging purposes.
        max_output (int): The maximum allowable length of the output.
        safe_mode (str): The safety mode setting for operation.
        disable_procedures (bool): Disables the use of certain procedures if set.
        conversation_history (bool): Enables recording of conversation history.
        conversation_filename (str | None): The filename to save the conversation history.
        conversation_history_path (str): Path where conversation history files are stored.
        model (str): Identifier for the language model used.
        temperature (float | None): Controls the randomness of response generation in the model.
        system_message (str): Custom message displayed by the system.
        context_window (int | None): Specifies the context size for the language model.
        max_tokens (int | None): Maximum tokens to generate in one response.
        api_base (str | None): Base URL for the API used by the language model.
        api_key (str | None): API key for authenticating requests to the language model.
        api_version (str | None): Specifies the version of the API used by the language model.
        max_budget (int | None): Budget limit for the language model's responses.
        _llm (object | None): A reference to the initialized language model.
        function_calling_llm (callable | None): The function used to invoke the language model.
        vision (bool): Indicates if the language model supports visual inputs.
    Methods:
        __init__(self):
            Constructor method that initializes the class attributes with default values and loads the configuration.
        start_terminal_interface(self):
            Initiates the terminal interface for user interaction.
        extend_config(self, config_path):
            Extends the current configuration with values from a provided file path.
        chat(self, message=None, display=True, stream=False):
            Handles chatting with the language model, either in streaming or pull mode.
        _streaming_chat(self, message=None, display=True):
            A generator function to manage streaming chat with the language model.
        _respond(self):
            A generator function that handles responses from the language model.
        reset(self):
            Resets the interpreter to its initial state, terminating any active interpreters.
        generate_system_message(self):
            Generates a system message using the current configuration and state.
    """
    def start_terminal_interface(self):
        """
            Starts a terminal-based user interface for interacting with the Open Interpreter.
            This function creates an `argparse.ArgumentParser` instance, adds various command-line
            arguments based on pre-defined settings, and parses incoming arguments. It then
            handles special command-line arguments like '--config' and '--conversations' to open
            config files and list or resume conversations, respectively. Depending on other
            arguments provided, it sets properties on the `interpreter`, performs local execution
            setup when requested, and initiates a conversation through the `interpreter.chat()`
            method. Special flags adjust interpreter configuration, enable safety mechanisms,
            trigger fast mode, enable vision features, and check for updates. If `--version` is
            specified, it displays the version of the Open Interpreter. Validates language model
            (LLM) settings before starting a chat session.
            Parameters:
                interpreter (Interpreter): An instance of the interpreter class which provides
                                           methods and attributes to handle language model
                                           interactions and maintain conversation state.
            Raises:
                FileNotFoundError: If a necessary file is not found during the process.
            Returns:
                None
        """
        start_terminal_interface(self)

    def __init__(self):
        """
        Constructor for initializing the main configuration and properties of the class.
        Attributes:
            messages (List[str]): A list to store messages.
            _code_interpreters (Dict): A dictionary to map code interpreters.
            config_file (str): Path to the user's configuration file.
            local (bool): A flag to determine local execution, defaults to False.
            auto_run (bool): A flag to trigger auto run feature, defaults to False.
            debug_mode (bool): A flag for enabling debug mode, defaults to False.
            max_output (int): Maximum allowed output size, defaults to 2000.
            safe_mode (str): Indicator of the safety mode, defaults to 'off'.
            disable_procedures (bool): A flag to disable procedures, defaults to False.
            conversation_history (bool): A flag to enable or disable conversation history, defaults to True.
            conversation_filename (str): The filename where the conversation history is stored.
            conversation_history_path (str): Path to the store for conversation histories.
            model (str): The model identifier for LLM.
            temperature (Optional[float]): Temperature setting for LLM, defaults to None.
            system_message (str): Default system message.
            context_window (Optional[int]): Size of the context window for the LLM, defaults to None.
            max_tokens (Optional[int]): Maximum number of tokens for the LLM, defaults to None.
            api_base (Optional[str]): Base url of the API used for LLM, defaults to None.
            api_key (Optional[str]): API key for accessing LLM, defaults to None.
            api_version (Optional[str]): API version of the LLM, defaults to None.
            max_budget (Optional[int]): Budget for calls to the LLM, defaults to None.
            _llm (Optional[object]): Placeholder for the LLM instance, defaults to None.
            function_calling_llm (Optional[callable]): Reference to a function that calls the LLM, defaults to None.
            vision (bool): A flag to indicate support for vision in LLM, defaults to False.
        Actions:
            - Sets initial attributes with default values or derived from a config file.
            - Initializes properties related to LLM configuration.
            - Loads and extends the configuration from the provided configuration file.
            - Exposes the Interpreter class for creating new instances.
        """
        # State
        self.messages = []
        self._code_interpreters = {}

        self.config_file = user_config_path

        # Settings
        self.local = False
        self.auto_run = False
        self.debug_mode = False
        self.max_output = 2000
        self.safe_mode = "off"
        self.disable_procedures = False
        # In the future, we'll use this to start with all languages
        # self.languages = [i.name for i in self.computer.interfaces]

        # Conversation history
        self.conversation_history = True
        self.conversation_filename = None
        self.conversation_history_path = get_storage_path("conversations")

        # LLM settings
        self.model = ""
        self.temperature = None
        self.system_message = ""
        self.context_window = None
        self.max_tokens = None
        self.api_base = None
        self.api_key = None
        self.api_version = None
        self.max_budget = None
        self._llm = None
        self.function_calling_llm = None
        self.vision = False  # LLM supports vision

        # Load config defaults
        self.extend_config(self.config_file)

        # Expose class so people can make new instances
        self.Interpreter = Interpreter

    def extend_config(self, config_path):
        """
            Extend the current configuration using an external config file.
            The external configuration file specified by the `config_path` parameter is loaded,
            and its contents are used to update the current instance's attributes.
            If the `debug_mode` of the instance is set to `True`, it prints a message about
            the source of the extended configuration.
            Args:
                config_path (str): The path to the external configuration file to be loaded.
            Raises:
                FileNotFoundError: If the `config_path` does not point to a valid file.
                yaml.YAMLError: If there is an error while parsing the YAML from the configuration file.
            Note:
                The `extend_config` method relies on the `get_config` function to load
                the external configuration file. It then uses the `update` method on the
                instance's `__dict__` to incorporate the loaded configuration.
        """
        if self.debug_mode:
            print(f"Extending configuration from `{config_path}`")

        config = get_config(config_path)
        self.__dict__.update(config)

    def chat(self, message=None, display=True, stream=False):
        """
        Handles the chat interactions, optionally streaming the output or pulling from the stream to update the messages list.
            This method is responsible for initiating a chat session. If the 'stream' argument is set to True, it calls
            the '_streaming_chat' method to handle real-time streaming of the chat. On the other hand, if 'stream' is False,
            it pulls the messages from the stream and updates the 'messages' list until the chat is complete.
            Args:
                message (str, optional): The initial message to start the chat. Defaults to None.
                display (bool, optional): Flag indicating whether to display the chat messages. Defaults to True.
                stream (bool, optional): Determines whether the chat will be streamed or pulled from the stream. Defaults to False.
            Returns:
                list: The 'messages' list containing all chat messages if 'stream' is False.
        """
        if stream:
            return self._streaming_chat(message=message, display=display)

        # If stream=False, *pull* from the stream.
        for _ in self._streaming_chat(message=message, display=display):
            pass

        return self.messages

    def _streaming_chat(self, message=None, display=True):
        # Setup the LLM
        if not self._llm:
            self._llm = setup_llm(self)

        # Sometimes a little more code -> a much better experience!
        # Display mode actually runs interpreter.chat(display=False, stream=True) from within the terminal_interface.
        # wraps the vanilla .chat(display=False) generator in a display.
        # Quite different from the plain generator stuff. So redirect to that
        if display:
            yield from terminal_interface(self, message)
            return

        # One-off message
        if message or message == "":
            if message == "":
                message = "No entry from user - please suggest something to enter."

            ## We support multiple formats for the incoming message:
            # Dict (these are passed directly in)
            if isinstance(message, dict):
                if "role" not in message:
                    message["role"] = "user"
                self.messages.append(message)
            # String (we construct a user message dict)
            elif isinstance(message, str):
                self.messages.append({"role": "user", "message": message})
            # List (this is like the OpenAI API)
            elif isinstance(message, list):
                self.messages = message

            yield from self._respond()

            # Save conversation if we've turned conversation_history on
            if self.conversation_history:
                # If it's the first message, set the conversation name
                if not self.conversation_filename:
                    first_few_words = "_".join(
                        self.messages[0]["message"][:25].split(" ")[:-1]
                    )
                    for char in '<>:"/\\|?*!':  # Invalid characters for filenames
                        first_few_words = first_few_words.replace(char, "")

                    date = datetime.now().strftime("%B_%d_%Y_%H-%M-%S")
                    self.conversation_filename = (
                        "__".join([first_few_words, date]) + ".json"
                    )

                # Check if the directory exists, if not, create it
                if not os.path.exists(self.conversation_history_path):
                    os.makedirs(self.conversation_history_path)
                # Write or overwrite the file
                with open(
                    os.path.join(
                        self.conversation_history_path, self.conversation_filename
                    ),
                    "w",
                ) as f:
                    json.dump(self.messages, f)
            return

        raise Exception(
            "`interpreter.chat()` requires a display. Set `display=True` or pass a message into `interpreter.chat(message)`."
        )

    def _respond(self):
        """
            Generator function that drives the interaction between the assistant and the LLM (Language Learning Model).
            This function invokes the respond method with the instance of the interpreter, managing
            the communication between system-generated messages and the interpreter's messages.
            It handles the stream of outputs, deals with exceptions, and manages code execution
            based on the executed language and messages processed by the LLM.
            While in a loop, the function listens for chunks of information from the LLM,
            merges messages, handles budget exceedance, and runs code if present.
            Upon code execution, it uses configured code interpreters, handles output line
            by line to the user, and deals with potential errors or exceptions.
            It terminates when there is no more code to execute or an unrecoverable error occurs.
            Yields:
                dict: A dictionary containing chunks of processed data,
                      signals of various states, or outputs during the interaction.
            Raises:
                Exception: Reraises exceptions that occur when dealing with API key issues,
                           local server issues, or any other irrecoverable errors.
        """
        yield from respond(self)

    def reset(self):
        """
            Reset internal state of the interpreter, including termination of any running code interpreters.
            This method cleans up the current state of all code interpreters managed by the instance by terminating each one.
            It also resets the internal mapping of code interpreters back to its initial empty state.
            Additionally, it restores the default `generate_system_message` method in case it was overridden by the user.
            Finally, it reinitializes the instance (reinvokes the `__init__` method) to ensure a fresh state.
            This method is intended to be used when a complete refresh of the interpreter's state is required,
            such as when restarting an interactive session or clearing out stale execution environments.
            Returns:
                None.
        """
        for code_interpreter in self._code_interpreters.values():
            code_interpreter.terminate()
        self._code_interpreters = {}

        # Reset the function below, in case the user set it
        self.generate_system_message = lambda: generate_system_message(self)

        self.__init__()

    # These functions are worth exposing to developers
    # I wish we could just dynamically expose all of our functions to devs...
    def generate_system_message(self):
        return generate_system_message(self)
